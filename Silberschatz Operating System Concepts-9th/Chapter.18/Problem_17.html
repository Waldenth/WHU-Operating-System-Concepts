<h2>Step-1</h2>
<div class="answer"> <p> <b>Completely Fair Scheduler:</b> </p> <p>This scheduler provides a choice of variable time slices, depending upon the process requirement. Similarly, priority is another core variable used by this scheduler. Hence, instead of allotting fixed time duration to each new process the scheduler designates a specific proportion of the processors time to each process undergoing execution. </p> <p> <b>Variables in CFS</b> </p> <p>CFS depends on two different variables for scheduling.</p> <p>• First variable is <b>Target Latency</b>; that is the length of time during which the process must run at least once. </p> <p>• Second one is <b>minimum granularity</b>; that is the minimum amount of time that is assigned to the process on the CPU.</p> <p>Depending on these two, a set of N processes that are to be executed; each process would be allotted to the processor for 1/N time duration. Whereas, priorities are to be allotted on the basis of process weight; the nice value of any process. </p> <p>The smaller it weighs, the higher becomes its priority; and vice-versa. </p> </div>
<h2>Step-2</h2>
<div class="answer"> <p> <b>Problems with extreme values of variables </b> </p> <p>This seems fair, but the problem might arise if the number of processes becomes excessively large. In that case CPU will have to switch the tasks very frequently as the target latency will be very small. This would make the process rather inefficient. </p> <p>This can be understood with a small example.</p> <p>Suppose the scheduler has queued two processes of equal weight and priority; the target latency being 20 milliseconds.</p> <p>In this case, initially the first process would run for 10 milliseconds and discontinue the execution and allows the second process to enter. Then the second process also runs for 10 milliseconds and it goes like this. </p> <p>Similarly, if the numbers of processes grow to 20 processes; each process would get a millisecond of execution span each time the target latency repeats itself. </p> </div>
<h2>Step-3</h2>
<div class="answer"> <p>In the above example suppose that there are 2000 processes that need to be served. Now the target latency would be very low and consequently the switching cost would be very high. </p> <p>This disadvantage can be overcome using the second variable; <b>minimum granularity</b>. This will ensure that the process runs at least for the specified time regardless of what the target latency is. </p> <p>Thus setting target latency to a very high value will decrease the switching cost but decreases the fairness too. Setting it to a very low value would increase the switching cost. </p> <p>Same is the thing with the minimum granularity. Higher values will lead to the decreased fairness as the processes will take long time to execute. The opposite would make the switching cost higher even if the number of processes is nominally limited. </p></div>
